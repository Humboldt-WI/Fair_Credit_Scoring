{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR IN-PROCESSING\n",
    "\n",
    "This notebook implements the Adersarial Debiasing in-processor [(Zhang et al. 2018)](https://dl.acm.org/doi/abs/10.1145/3278721.3278779).\n",
    "\n",
    "The modeling is performed separately for each combination of training folds. This is controlled with `use_fold` variable. To fit adervsarial debiasing on a different combination of training folds, set `use_fold` to a specific value and restar the kernel.\n",
    "\n",
    "A further analysis of the processor outputs is performed in `code_05_inprocess3.R`.\n",
    "\n",
    "The notebook loads the data exported in `code_00_partitinoing.ipynb` and applies pre-processors. The processor predictions are exported as CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# working paths\n",
    "%run 'code_00_working_paths.py'\n",
    "\n",
    "# sepcify data set\n",
    "# one of ['bene', 'german', 'uk', 'taiwan', 'pkdd', 'gmsc', 'homecredit']\n",
    "data = 'taiwan'\n",
    "\n",
    "# partitioning\n",
    "num_folds = 5\n",
    "use_fold  = 4 # one of [0, 1, ..., num_folds-1]\n",
    "seed      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IN-PROCESSOR PARAMS\n",
    "\n",
    "adversary_loss_weight = 0.1 # other options: [0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from load_data import *\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 186)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD PARTITIONING\n",
    "\n",
    "dataset_orig_test = pickle.load(open(data_path + 'prepared/' + data + '_orig_test.pkl', 'rb'))\n",
    "te                = dataset_orig_test.convert_to_dataframe()[0]\n",
    "\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'AGE'\n",
    "privileged_groups   = [{'AGE': 1}] \n",
    "unprivileged_groups = [{'AGE': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- FOLD 4...\n",
      "------------------------------\n",
      "WARNING:tensorflow:From H:/Fair Credit Scoring [revision]/functions\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:133: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From H:/Fair Credit Scoring [revision]/functions\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:137: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From H:/Fair Credit Scoring [revision]/functions\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:82: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From H:/Fair Credit Scoring [revision]/functions\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:87: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From H:/Fair Credit Scoring [revision]/functions\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:155: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From H:/Fair Credit Scoring [revision]/functions\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:157: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From H:/Fair Credit Scoring [revision]/functions\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:161: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From H:/Fair Credit Scoring [revision]/functions\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:182: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From H:/Fair Credit Scoring [revision]/functions\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:183: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "epoch 0; iter: 0; batch classifier loss: 0.692659; batch adversarial loss: 0.923862\n",
      "epoch 0; iter: 200; batch classifier loss: 0.604467; batch adversarial loss: 0.841103\n",
      "epoch 1; iter: 0; batch classifier loss: 0.480069; batch adversarial loss: 0.873279\n",
      "epoch 1; iter: 200; batch classifier loss: 0.540533; batch adversarial loss: 0.658383\n",
      "epoch 2; iter: 0; batch classifier loss: 0.520599; batch adversarial loss: 0.651513\n",
      "epoch 2; iter: 200; batch classifier loss: 0.595018; batch adversarial loss: 0.502645\n",
      "epoch 3; iter: 0; batch classifier loss: 0.508991; batch adversarial loss: 0.531144\n",
      "epoch 3; iter: 200; batch classifier loss: 0.593834; batch adversarial loss: 0.460085\n",
      "epoch 4; iter: 0; batch classifier loss: 0.533690; batch adversarial loss: 0.479645\n",
      "epoch 4; iter: 200; batch classifier loss: 0.575705; batch adversarial loss: 0.428333\n",
      "epoch 5; iter: 0; batch classifier loss: 0.511087; batch adversarial loss: 0.373482\n",
      "epoch 5; iter: 200; batch classifier loss: 0.545365; batch adversarial loss: 0.396340\n",
      "epoch 6; iter: 0; batch classifier loss: 0.512255; batch adversarial loss: 0.425453\n",
      "epoch 6; iter: 200; batch classifier loss: 0.566254; batch adversarial loss: 0.386051\n",
      "epoch 7; iter: 0; batch classifier loss: 0.596714; batch adversarial loss: 0.422885\n",
      "epoch 7; iter: 200; batch classifier loss: 0.572485; batch adversarial loss: 0.338302\n",
      "epoch 8; iter: 0; batch classifier loss: 0.493706; batch adversarial loss: 0.359009\n",
      "epoch 8; iter: 200; batch classifier loss: 0.576831; batch adversarial loss: 0.357822\n",
      "epoch 9; iter: 0; batch classifier loss: 0.505702; batch adversarial loss: 0.359868\n",
      "epoch 9; iter: 200; batch classifier loss: 0.469925; batch adversarial loss: 0.439909\n",
      "epoch 10; iter: 0; batch classifier loss: 0.551659; batch adversarial loss: 0.370775\n",
      "epoch 10; iter: 200; batch classifier loss: 0.577093; batch adversarial loss: 0.435390\n",
      "epoch 11; iter: 0; batch classifier loss: 0.491934; batch adversarial loss: 0.337361\n",
      "epoch 11; iter: 200; batch classifier loss: 0.534861; batch adversarial loss: 0.314348\n",
      "epoch 12; iter: 0; batch classifier loss: 0.553856; batch adversarial loss: 0.349578\n",
      "epoch 12; iter: 200; batch classifier loss: 0.504451; batch adversarial loss: 0.422444\n",
      "epoch 13; iter: 0; batch classifier loss: 0.550322; batch adversarial loss: 0.331413\n",
      "epoch 13; iter: 200; batch classifier loss: 0.531051; batch adversarial loss: 0.333177\n",
      "epoch 14; iter: 0; batch classifier loss: 0.483073; batch adversarial loss: 0.295656\n",
      "epoch 14; iter: 200; batch classifier loss: 0.586147; batch adversarial loss: 0.385647\n",
      "epoch 15; iter: 0; batch classifier loss: 0.489024; batch adversarial loss: 0.294656\n",
      "epoch 15; iter: 200; batch classifier loss: 0.535444; batch adversarial loss: 0.532056\n",
      "epoch 16; iter: 0; batch classifier loss: 0.582391; batch adversarial loss: 0.480748\n",
      "epoch 16; iter: 200; batch classifier loss: 0.481028; batch adversarial loss: 0.443382\n",
      "epoch 17; iter: 0; batch classifier loss: 0.563985; batch adversarial loss: 0.326756\n",
      "epoch 17; iter: 200; batch classifier loss: 0.528387; batch adversarial loss: 0.328106\n",
      "epoch 18; iter: 0; batch classifier loss: 0.441756; batch adversarial loss: 0.354401\n",
      "epoch 18; iter: 200; batch classifier loss: 0.465634; batch adversarial loss: 0.368448\n",
      "epoch 19; iter: 0; batch classifier loss: 0.509989; batch adversarial loss: 0.470130\n",
      "epoch 19; iter: 200; batch classifier loss: 0.491110; batch adversarial loss: 0.362520\n",
      "epoch 20; iter: 0; batch classifier loss: 0.568904; batch adversarial loss: 0.355190\n",
      "epoch 20; iter: 200; batch classifier loss: 0.492376; batch adversarial loss: 0.271977\n",
      "epoch 21; iter: 0; batch classifier loss: 0.556390; batch adversarial loss: 0.370664\n",
      "epoch 21; iter: 200; batch classifier loss: 0.548568; batch adversarial loss: 0.288148\n",
      "epoch 22; iter: 0; batch classifier loss: 0.564717; batch adversarial loss: 0.441833\n",
      "epoch 22; iter: 200; batch classifier loss: 0.449275; batch adversarial loss: 0.332129\n",
      "epoch 23; iter: 0; batch classifier loss: 0.502794; batch adversarial loss: 0.434791\n",
      "epoch 23; iter: 200; batch classifier loss: 0.540894; batch adversarial loss: 0.328435\n",
      "epoch 24; iter: 0; batch classifier loss: 0.447500; batch adversarial loss: 0.479329\n",
      "epoch 24; iter: 200; batch classifier loss: 0.552448; batch adversarial loss: 0.400746\n",
      "epoch 25; iter: 0; batch classifier loss: 0.501177; batch adversarial loss: 0.384576\n",
      "epoch 25; iter: 200; batch classifier loss: 0.484638; batch adversarial loss: 0.367104\n",
      "epoch 26; iter: 0; batch classifier loss: 0.454103; batch adversarial loss: 0.354863\n",
      "epoch 26; iter: 200; batch classifier loss: 0.434778; batch adversarial loss: 0.338263\n",
      "epoch 27; iter: 0; batch classifier loss: 0.461825; batch adversarial loss: 0.339514\n",
      "epoch 27; iter: 200; batch classifier loss: 0.438563; batch adversarial loss: 0.365160\n",
      "epoch 28; iter: 0; batch classifier loss: 0.569997; batch adversarial loss: 0.387411\n",
      "epoch 28; iter: 200; batch classifier loss: 0.462226; batch adversarial loss: 0.362004\n",
      "epoch 29; iter: 0; batch classifier loss: 0.549453; batch adversarial loss: 0.354625\n",
      "epoch 29; iter: 200; batch classifier loss: 0.368541; batch adversarial loss: 0.365856\n",
      "epoch 30; iter: 0; batch classifier loss: 0.468573; batch adversarial loss: 0.329672\n",
      "epoch 30; iter: 200; batch classifier loss: 0.460263; batch adversarial loss: 0.343265\n",
      "epoch 31; iter: 0; batch classifier loss: 0.446159; batch adversarial loss: 0.323619\n",
      "epoch 31; iter: 200; batch classifier loss: 0.476972; batch adversarial loss: 0.233612\n",
      "epoch 32; iter: 0; batch classifier loss: 0.397365; batch adversarial loss: 0.350990\n",
      "epoch 32; iter: 200; batch classifier loss: 0.475326; batch adversarial loss: 0.299766\n",
      "epoch 33; iter: 0; batch classifier loss: 0.495485; batch adversarial loss: 0.419742\n",
      "epoch 33; iter: 200; batch classifier loss: 0.502702; batch adversarial loss: 0.405894\n",
      "epoch 34; iter: 0; batch classifier loss: 0.475635; batch adversarial loss: 0.312228\n",
      "epoch 34; iter: 200; batch classifier loss: 0.485407; batch adversarial loss: 0.382485\n",
      "epoch 35; iter: 0; batch classifier loss: 0.486744; batch adversarial loss: 0.323738\n",
      "epoch 35; iter: 200; batch classifier loss: 0.457597; batch adversarial loss: 0.309721\n",
      "epoch 36; iter: 0; batch classifier loss: 0.497541; batch adversarial loss: 0.361812\n",
      "epoch 36; iter: 200; batch classifier loss: 0.468637; batch adversarial loss: 0.388475\n",
      "epoch 37; iter: 0; batch classifier loss: 0.413470; batch adversarial loss: 0.334698\n",
      "epoch 37; iter: 200; batch classifier loss: 0.424606; batch adversarial loss: 0.354649\n",
      "epoch 38; iter: 0; batch classifier loss: 0.397561; batch adversarial loss: 0.314451\n",
      "epoch 38; iter: 200; batch classifier loss: 0.485078; batch adversarial loss: 0.359131\n",
      "epoch 39; iter: 0; batch classifier loss: 0.355848; batch adversarial loss: 0.341765\n",
      "epoch 39; iter: 200; batch classifier loss: 0.501323; batch adversarial loss: 0.327781\n",
      "epoch 40; iter: 0; batch classifier loss: 0.463290; batch adversarial loss: 0.273438\n",
      "epoch 40; iter: 200; batch classifier loss: 0.457137; batch adversarial loss: 0.329775\n",
      "epoch 41; iter: 0; batch classifier loss: 0.520469; batch adversarial loss: 0.387166\n",
      "epoch 41; iter: 200; batch classifier loss: 0.454205; batch adversarial loss: 0.402468\n",
      "epoch 42; iter: 0; batch classifier loss: 0.441355; batch adversarial loss: 0.449823\n",
      "epoch 42; iter: 200; batch classifier loss: 0.459601; batch adversarial loss: 0.416586\n",
      "epoch 43; iter: 0; batch classifier loss: 0.442792; batch adversarial loss: 0.329739\n",
      "epoch 43; iter: 200; batch classifier loss: 0.429738; batch adversarial loss: 0.310499\n",
      "epoch 44; iter: 0; batch classifier loss: 0.400599; batch adversarial loss: 0.353751\n",
      "epoch 44; iter: 200; batch classifier loss: 0.503953; batch adversarial loss: 0.333737\n",
      "epoch 45; iter: 0; batch classifier loss: 0.457727; batch adversarial loss: 0.364564\n",
      "epoch 45; iter: 200; batch classifier loss: 0.358802; batch adversarial loss: 0.437394\n",
      "epoch 46; iter: 0; batch classifier loss: 0.458296; batch adversarial loss: 0.311709\n",
      "epoch 46; iter: 200; batch classifier loss: 0.422124; batch adversarial loss: 0.397391\n",
      "epoch 47; iter: 0; batch classifier loss: 0.470824; batch adversarial loss: 0.439781\n",
      "epoch 47; iter: 200; batch classifier loss: 0.454111; batch adversarial loss: 0.354038\n",
      "epoch 48; iter: 0; batch classifier loss: 0.542221; batch adversarial loss: 0.283902\n",
      "epoch 48; iter: 200; batch classifier loss: 0.431037; batch adversarial loss: 0.268445\n",
      "epoch 49; iter: 0; batch classifier loss: 0.468131; batch adversarial loss: 0.353314\n",
      "epoch 49; iter: 200; batch classifier loss: 0.413838; batch adversarial loss: 0.252764\n",
      "\n",
      "Finished in 0.40 minutes\n"
     ]
    }
   ],
   "source": [
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# loop through training folds\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### LOAD DATA\n",
    "    \n",
    "    # select fold combination\n",
    "    if fold != use_fold:\n",
    "        continue\n",
    "    \n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # import data\n",
    "    data_train = pickle.load(open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_train.pkl', 'rb'))\n",
    "    data_valid = pickle.load(open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_valid.pkl', 'rb'))\n",
    "    data_test  = pickle.load(open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_test.pkl',  'rb'))\n",
    "    \n",
    "\n",
    "    ##### MODELING\n",
    "\n",
    "    # start tensorflow session\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # fit adversarial debiasing\n",
    "    debiased_model = AdversarialDebiasing(privileged_groups     = privileged_groups,\n",
    "                                          unprivileged_groups   = unprivileged_groups,\n",
    "                                          debias                = True,\n",
    "                                          adversary_loss_weight = adversary_loss_weight,\n",
    "                                          scope_name            = 'debiased_classifier',\n",
    "                                          sess                  = sess)\n",
    "    debiased_model.fit(data_train)\n",
    "    \n",
    "    # apply the model to valid data\n",
    "    scores_valid = debiased_model.predict(data_valid).scores\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores']  = scores_valid\n",
    "    advdebias_predictions['targets'] = data_valid.labels.flatten()\n",
    "    advdebias_predictions.to_csv(res_path + 'intermediate/' + data + '_' + str(fold) + '_AD_' + str(adversary_loss_weight) + '_predictions_valid.csv', \n",
    "                                 index  = None, \n",
    "                                 header = True)\n",
    "    \n",
    "    # apply the model to test data\n",
    "    scores_test = debiased_model.predict(data_test).scores\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores'] = scores_test\n",
    "    advdebias_predictions.to_csv(res_path + 'intermediate/' + data + '_' + str(fold) + '_AD_' + str(adversary_loss_weight) + '_predictions_test.csv', \n",
    "                                 index  = None, \n",
    "                                 header = True)\n",
    "    \n",
    "    # print performance\n",
    "    print('')\n",
    "    print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
