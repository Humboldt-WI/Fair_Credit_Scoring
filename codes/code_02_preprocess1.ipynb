{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR PRE-PROCESSING\n",
    "\n",
    "This notebook implements the following pre-processors:\n",
    "- reweighing\n",
    "- disparate impact remover\n",
    "\n",
    "A further analysis of the processor outputs is performed in `code_02_preprocess2.R` and `code_03_preprocess3.R`.\n",
    "\n",
    "The notebook loads the data exported in `code_00_partitinoing.ipynb` and applies pre-processors. The transformed training and test data is exported as CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# working path\n",
    "path      = 'H:/Fair Credit Scoring/'\n",
    "func_path = path + 'functions/'\n",
    "data_path = path + 'data/'\n",
    "res_path  = path + 'results/'\n",
    "out_path  = path + 'output/'\n",
    "\n",
    "# data  set\n",
    "# one of ['bene', 'german', 'uk', 'taiwan', 'pkdd', 'gmsc', 'homecredit']\n",
    "data = 'taiwan' \n",
    "\n",
    "# partitioning\n",
    "num_folds  = 5\n",
    "seed       = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PRE-PROCESSOR PARAMS\n",
    "\n",
    "all_lambda = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba==0.48 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (0.48.0)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from numba==0.48) (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from numba==0.48) (1.19.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from numba==0.48) (52.0.0.post20210125)\n",
      "Requirement already satisfied: BlackBoxAuditing in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (0.1.54)\n",
      "Requirement already satisfied: pandas in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from BlackBoxAuditing) (1.2.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from BlackBoxAuditing) (2.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from BlackBoxAuditing) (3.3.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from BlackBoxAuditing) (1.19.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (8.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from cycler>=0.10->matplotlib->BlackBoxAuditing) (1.15.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from networkx->BlackBoxAuditing) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from pandas->BlackBoxAuditing) (2021.1)\n"
     ]
    }
   ],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from load_data import *\n",
    "\n",
    "!pip install numba==0.48\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install BlackBoxAuditing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 186)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD PARTITIONING\n",
    "\n",
    "dataset_orig_test = pickle.load(open(data_path + 'prepared/' + data + '_orig_test.pkl', 'rb'))\n",
    "te                = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'AGE'\n",
    "privileged_groups   = [{'AGE': 1}] \n",
    "unprivileged_groups = [{'AGE': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- METHOD: RW...\n",
      "------------------------------\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = 0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "\n",
      "------------------------------\n",
      "- METHOD: DI...\n",
      "------------------------------\n",
      "-- FOLD 0...\n",
      "-- FOLD 1...\n",
      "-- FOLD 2...\n",
      "-- FOLD 3...\n",
      "-- FOLD 4...\n",
      "\n",
      "\n",
      "Finished in 27.54 minutes\n"
     ]
    }
   ],
   "source": [
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# list of processors\n",
    "methods = ['RW', 'DI']\n",
    "\n",
    "# processing loop\n",
    "for m in methods:\n",
    "    \n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- METHOD: ' + m + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # loop through fold combinations\n",
    "    for fold in range(num_folds):\n",
    "    \n",
    "        ##### LOAD DATA\n",
    "\n",
    "        # import data subsets\n",
    "        data_train = pickle.load(open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_train.pkl', 'rb'))\n",
    "        data_valid = pickle.load(open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_valid.pkl', 'rb'))\n",
    "        data_test  = pickle.load(open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_test.pkl',  'rb'))\n",
    "            \n",
    "    \n",
    "        ##### MODELING\n",
    "    \n",
    "        # reweighing\n",
    "        if m == 'RW':\n",
    "            \n",
    "            # fit pre-processor\n",
    "            RW = Reweighing(unprivileged_groups = unprivileged_groups,\n",
    "                            privileged_groups   = privileged_groups)\n",
    "            RW.fit(data_train)\n",
    "\n",
    "            # train processing\n",
    "            dataset_transf_train = RW.transform(data_train)\n",
    "            w_train   = dataset_transf_train.instance_weights.ravel()\n",
    "            out_train = dataset_transf_train.convert_to_dataframe()[0]\n",
    "            out_train = out_train.sample(n = out_train.shape[0], replace = True, weights = w_train)\n",
    "\n",
    "            # valid classification\n",
    "            dataset_transf_valid = RW.transform(data_valid)\n",
    "            w_valid   = dataset_transf_valid.instance_weights.ravel()\n",
    "            out_valid = dataset_transf_valid.convert_to_dataframe()[0]\n",
    "            out_valid = out_valid.sample(n = out_valid.shape[0], replace = True, weights = w_valid)\n",
    "\n",
    "            # test processing\n",
    "            dataset_transf_test = RW.transform(data_test)\n",
    "            w_test   = dataset_transf_test.instance_weights.ravel()\n",
    "            out_test = dataset_transf_test.convert_to_dataframe()[0]\n",
    "            out_test = out_test.sample(n = out_test.shape[0], replace = True, weights = w_test)\n",
    "\n",
    "            # check transformation\n",
    "            assert np.abs(dataset_transf_train.instance_weights.sum() - data_train.instance_weights.sum()) < 1e-6\n",
    "\n",
    "            # check results\n",
    "            metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                                           unprivileged_groups = unprivileged_groups,\n",
    "                                                           privileged_groups   = privileged_groups)\n",
    "            print('-- achieved a statistical parity difference between unprivileged and privileged groups = %f' % metric_transf_train.mean_difference())\n",
    "            \n",
    "            # export CSV\n",
    "            out_train.to_csv(data_path + 'prepared/' + data + '_' + str(fold) + '_pre_train_' + m + '.csv', index = None, header = True)\n",
    "            out_valid.to_csv(data_path + 'prepared/' + data + '_' + str(fold) + '_pre_valid_' + m + '.csv', index = None, header = True)\n",
    "            out_test.to_csv(data_path + 'prepared/'  + data + '_' + str(fold) + '_pre_test_'  + m + '.csv', index = None, header = True)\n",
    "\n",
    "\n",
    "        # disparate impact remover\n",
    "        if m == 'DI':     \n",
    "            \n",
    "            # feedback\n",
    "            print('-- FOLD ' + str(fold) + '...')\n",
    "            \n",
    "            # loop through different repair levels\n",
    "            for i in all_lambda:\n",
    "                \n",
    "                # fit pre-processor\n",
    "                di = DisparateImpactRemover(repair_level = i, sensitive_attribute = protected)\n",
    "                di.fit(data_train)\n",
    "                                          \n",
    "                # train processing\n",
    "                dataset_transf_train = di.fit_transform(data_train)\n",
    "                out_train = dataset_transf_train.convert_to_dataframe()[0]\n",
    "\n",
    "                # valid processing\n",
    "                dataset_transf_valid = di.fit_transform(data_valid)\n",
    "                out_valid = dataset_transf_valid.convert_to_dataframe()[0]\n",
    "\n",
    "                # test processing\n",
    "                dataset_transf_test = di.fit_transform(data_test)\n",
    "                out_test = dataset_transf_test.convert_to_dataframe()[0]\n",
    "               \n",
    "                # export CSV\n",
    "                out_train.to_csv(data_path + 'prepared/' + data + '_' + str(fold) + '_pre_train_' + m + '_' + str(i) + '.csv', index = None, header = True)\n",
    "                out_valid.to_csv(data_path + 'prepared/' + data + '_' + str(fold) + '_pre_valid_' + m + '_' + str(i) + '.csv', index = None, header = True)\n",
    "                out_test.to_csv(data_path  + 'prepared/' + data + '_' + str(fold) + '_pre_test_'  + m + '_' + str(i) + '.csv', index = None, header = True)\n",
    "        \n",
    "    # feedback\n",
    "    print('')\n",
    "    \n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
