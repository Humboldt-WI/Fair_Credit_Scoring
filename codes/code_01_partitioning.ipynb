{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PARTITIONING\n",
    "\n",
    "Given a raw data set, this notebook performs the following operations:\n",
    "- randomly partitons a raw data set into training and test set\n",
    "- further splits the training set into training and validation fold combinations\n",
    "- applies `MaxAbsScaler` and exports scaled and raw data as CSV and pickle files\n",
    "\n",
    "This ensures that fairness processors trained in other notebooks use the same data partititioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# working path\n",
    "path      = 'H:/Fair Credit Scoring/'\n",
    "func_path = path + 'functions/'\n",
    "data_path = path + 'data/'\n",
    "res_path  = path + 'results/'\n",
    "out_path  = path + 'output/'\n",
    "\n",
    "# data  set\n",
    "# one of ['bene', 'german', 'uk', 'taiwan', 'pkdd', 'gmsc', 'homecredit']\n",
    "data = 'taiwan' \n",
    "\n",
    "# partitioning\n",
    "test_ratio = 0.3\n",
    "num_folds  = 5\n",
    "seed       = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba==0.48\n",
      "  Downloading numba-0.48.0-1-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from numba==0.48) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from numba==0.48) (1.19.2)\n",
      "Collecting llvmlite<0.32.0,>=0.31.0dev0\n",
      "  Downloading llvmlite-0.31.0-cp38-cp38-win_amd64.whl (13.6 MB)\n",
      "Installing collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.31.0 numba-0.48.0\n",
      "Collecting BlackBoxAuditing\n",
      "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from BlackBoxAuditing) (3.3.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from BlackBoxAuditing) (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from BlackBoxAuditing) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (8.1.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from cycler>=0.10->matplotlib->BlackBoxAuditing) (1.15.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from networkx->BlackBoxAuditing) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from pandas->BlackBoxAuditing) (2021.1)\n",
      "Building wheels for collected packages: BlackBoxAuditing\n",
      "  Building wheel for BlackBoxAuditing (setup.py): started\n",
      "  Building wheel for BlackBoxAuditing (setup.py): finished with status 'done'\n",
      "  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394769 sha256=e4380eb6de69f782d16bbfd18a8edec57fec2bef06b11be06f7f79dd0203cfa5\n",
      "  Stored in directory: c:\\users\\kozodoin4.hub.rdc\\appdata\\local\\pip\\cache\\wheels\\e3\\77\\36\\a32ec1b04c2ebe2c45e88d42f33f22f987e76aad3f297b681e\n",
      "Successfully built BlackBoxAuditing\n",
      "Installing collected packages: networkx, BlackBoxAuditing\n",
      "Successfully installed BlackBoxAuditing-0.1.54 networkx-2.5.1\n"
     ]
    }
   ],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from load_data import *\n",
    "\n",
    "!pip install numba==0.48\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "!pip install BlackBoxAuditing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA IMPORT\n",
    "\n",
    "dataset_orig = load_dataset(path = data_path + 'raw/' + data + '.csv', \n",
    "                            data = data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 186)\n"
     ]
    }
   ],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'AGE'\n",
    "privileged_groups   = [{'AGE': 1}] \n",
    "unprivileged_groups = [{'AGE': 0}]\n",
    "\n",
    "# check dimensions\n",
    "print(dataset_orig.metadata['params']['df'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partitioning and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 186) (15000, 186)\n"
     ]
    }
   ],
   "source": [
    "##### PARTITIONING\n",
    "\n",
    "# set seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# train / test partitioning\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([1 - test_ratio], shuffle = True)\n",
    "tr = dataset_orig_train.convert_to_dataframe()[0]\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    " \n",
    "# export test set\n",
    "te.to_csv(data_path + 'prepared/' + data + '_' + 'orig_test' + '.csv', index = None, header = True)\n",
    "pickle.dump(dataset_orig_test, open(data_path + 'prepared/' + data + '_orig_test.pkl',  'wb'))\n",
    "print(tr.shape, te.shape)\n",
    "\n",
    "# cross-validation on the training set\n",
    "skf = dataset_orig_train.split(num_or_size_splits = num_folds, seed = seed, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- (28000, 186) (7000, 186)\n",
      "-- (28000, 186) (7000, 186)\n",
      "-- (28000, 186) (7000, 186)\n",
      "-- (28000, 186) (7000, 186)\n",
      "-- (28000, 186) (7000, 186)\n",
      "\n",
      "Finished in 1.90 minutes\n"
     ]
    }
   ],
   "source": [
    "##### SCALING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# data partitioning loop\n",
    "for fold in range(num_folds):\n",
    "\n",
    "    ##### DATA PARTITIONING\n",
    "\n",
    "    # validation fold\n",
    "    data_valid = skf[fold].copy()\n",
    "\n",
    "    # train folds\n",
    "    train_folds = [f for f in range(num_folds) if f != fold]\n",
    "    for fold_idx in train_folds:\n",
    "\n",
    "        if fold_idx == train_folds[0]:\n",
    "            data_train = skf[fold_idx].copy()\n",
    "        else:\n",
    "            data_train.features             = np.concatenate([data_train.features, skf[fold_idx].features],                         axis = 0)\n",
    "            data_train.instance_names       = np.concatenate([data_train.instance_names, skf[fold_idx].instance_names],             axis = 0)\n",
    "            data_train.instance_weights     = np.concatenate([data_train.instance_weights, skf[fold_idx].instance_weights],         axis = 0)\n",
    "            data_train.labels               = np.concatenate([data_train.labels, skf[fold_idx].labels],                             axis = 0)\n",
    "            data_train.protected_attributes = np.concatenate([data_train.protected_attributes, skf[fold_idx].protected_attributes], axis = 0)\n",
    "            data_train.scores               = np.concatenate([data_train.scores, skf[fold_idx].scores],                             axis = 0)\n",
    "\n",
    "    # test set\n",
    "    data_test = dataset_orig_test.copy()\n",
    "    \n",
    "    # convert to DF\n",
    "    tr = data_train.convert_to_dataframe()[0]\n",
    "    va = data_valid.convert_to_dataframe()[0]\n",
    "\n",
    "    # export CSV\n",
    "    tr.to_csv(data_path + 'prepared/' + data + '_' + 'orig_' + str(fold) + '_train' + '.csv', index = None, header=True)\n",
    "    va.to_csv(data_path + 'prepared/' + data + '_' + 'orig_' + str(fold) + '_valid' + '.csv', index = None, header=True)\n",
    "    \n",
    "    # export pickle\n",
    "    pickle.dump(data_train, open(data_path + 'prepared/' + data + '_orig_' + str(fold) + '_train.pkl', 'wb'))\n",
    "    pickle.dump(data_valid, open(data_path + 'prepared/' + data + '_orig_' + str(fold) + '_valid.pkl', 'wb'))\n",
    "    print('--', tr.shape, va.shape)\n",
    "\n",
    "\n",
    "    ##### SCALING\n",
    "\n",
    "    # scale features\n",
    "    min_max_scaler      = MaxAbsScaler()\n",
    "    data_train.features = min_max_scaler.fit_transform(data_train.features)\n",
    "    data_valid.features = min_max_scaler.transform(data_valid.features)\n",
    "    data_test.features  = min_max_scaler.transform(data_test.features)\n",
    "\n",
    "    # convert to DF\n",
    "    tr = data_train.convert_to_dataframe()[0]\n",
    "    va = data_valid.convert_to_dataframe()[0]\n",
    "    te = data_test.convert_to_dataframe()[0]\n",
    "\n",
    "    # save CSV\n",
    "    tr.to_csv(data_path + 'prepared/' + data + '_' + 'scaled_' + str(fold) + '_train' + '.csv', index = None, header = True)\n",
    "    va.to_csv(data_path + 'prepared/' + data + '_' + 'scaled_' + str(fold) + '_valid' + '.csv', index = None, header = True)\n",
    "    te.to_csv(data_path + 'prepared/' + data + '_' + 'scaled_' + str(fold) + '_test'  + '.csv', index = None, header = True)\n",
    "    \n",
    "    # save pickle\n",
    "    pickle.dump(data_train, open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_train.pkl', 'wb'))\n",
    "    pickle.dump(data_valid, open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_valid.pkl', 'wb'))\n",
    "    pickle.dump(data_test,  open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_test.pkl',  'wb'))\n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
