{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PARTITIONING\n",
    "\n",
    "Given a raw data set, this notebook performs the following operations:\n",
    "- randomly partitons a raw data set into training and test set\n",
    "- further splits the training set into training and validation fold combinations\n",
    "- applies `MaxAbsScaler` and exports scaled and raw data as CSV and pickle files\n",
    "\n",
    "This ensures that fairness processors trained in other notebooks use the same data partititioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# working paths\n",
    "%run 'code_00_working_paths.py'\n",
    "\n",
    "# specify data set\n",
    "# one of ['bene', 'german', 'uk', 'taiwan', 'pkdd', 'gmsc', 'homecredit']\n",
    "data = 'taiwan' \n",
    "\n",
    "# partitioning\n",
    "test_ratio = 0.3\n",
    "num_folds  = 5\n",
    "seed       = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### IMPORTS\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from load_data import *\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA IMPORT\n",
    "\n",
    "dataset_orig = load_dataset(path = data_path + 'raw/' + data + '.csv', \n",
    "                            data = data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 186)\n"
     ]
    }
   ],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'AGE'\n",
    "privileged_groups   = [{'AGE': 1}] \n",
    "unprivileged_groups = [{'AGE': 0}]\n",
    "\n",
    "# check dimensions\n",
    "print(dataset_orig.metadata['params']['df'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partitioning and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 186) (15000, 186)\n"
     ]
    }
   ],
   "source": [
    "##### PARTITIONING\n",
    "\n",
    "# set seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# train / test partitioning\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([1 - test_ratio], shuffle = True)\n",
    "tr = dataset_orig_train.convert_to_dataframe()[0]\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    " \n",
    "# export test set\n",
    "te.to_csv(data_path + 'prepared/' + data + '_' + 'orig_test' + '.csv', index = None, header = True)\n",
    "pickle.dump(dataset_orig_test, open(data_path + 'prepared/' + data + '_orig_test.pkl',  'wb'))\n",
    "print(tr.shape, te.shape)\n",
    "\n",
    "# cross-validation on the training set\n",
    "skf = dataset_orig_train.split(num_or_size_splits = num_folds, seed = seed, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- (28000, 186) (7000, 186)\n",
      "-- (28000, 186) (7000, 186)\n",
      "-- (28000, 186) (7000, 186)\n",
      "-- (28000, 186) (7000, 186)\n",
      "-- (28000, 186) (7000, 186)\n",
      "\n",
      "Finished in 1.90 minutes\n"
     ]
    }
   ],
   "source": [
    "##### SCALING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# data partitioning loop\n",
    "for fold in range(num_folds):\n",
    "\n",
    "    ##### DATA PARTITIONING\n",
    "\n",
    "    # validation fold\n",
    "    data_valid = skf[fold].copy()\n",
    "\n",
    "    # train folds\n",
    "    train_folds = [f for f in range(num_folds) if f != fold]\n",
    "    for fold_idx in train_folds:\n",
    "\n",
    "        if fold_idx == train_folds[0]:\n",
    "            data_train = skf[fold_idx].copy()\n",
    "        else:\n",
    "            data_train.features             = np.concatenate([data_train.features, skf[fold_idx].features],                         axis = 0)\n",
    "            data_train.instance_names       = np.concatenate([data_train.instance_names, skf[fold_idx].instance_names],             axis = 0)\n",
    "            data_train.instance_weights     = np.concatenate([data_train.instance_weights, skf[fold_idx].instance_weights],         axis = 0)\n",
    "            data_train.labels               = np.concatenate([data_train.labels, skf[fold_idx].labels],                             axis = 0)\n",
    "            data_train.protected_attributes = np.concatenate([data_train.protected_attributes, skf[fold_idx].protected_attributes], axis = 0)\n",
    "            data_train.scores               = np.concatenate([data_train.scores, skf[fold_idx].scores],                             axis = 0)\n",
    "\n",
    "    # test set\n",
    "    data_test = dataset_orig_test.copy()\n",
    "    \n",
    "    # convert to DF\n",
    "    tr = data_train.convert_to_dataframe()[0]\n",
    "    va = data_valid.convert_to_dataframe()[0]\n",
    "\n",
    "    # export CSV\n",
    "    tr.to_csv(data_path + 'prepared/' + data + '_' + 'orig_' + str(fold) + '_train' + '.csv', index = None, header=True)\n",
    "    va.to_csv(data_path + 'prepared/' + data + '_' + 'orig_' + str(fold) + '_valid' + '.csv', index = None, header=True)\n",
    "    \n",
    "    # export pickle\n",
    "    pickle.dump(data_train, open(data_path + 'prepared/' + data + '_orig_' + str(fold) + '_train.pkl', 'wb'))\n",
    "    pickle.dump(data_valid, open(data_path + 'prepared/' + data + '_orig_' + str(fold) + '_valid.pkl', 'wb'))\n",
    "    print('--', tr.shape, va.shape)\n",
    "\n",
    "\n",
    "    ##### SCALING\n",
    "\n",
    "    # scale features\n",
    "    min_max_scaler      = MaxAbsScaler()\n",
    "    data_train.features = min_max_scaler.fit_transform(data_train.features)\n",
    "    data_valid.features = min_max_scaler.transform(data_valid.features)\n",
    "    data_test.features  = min_max_scaler.transform(data_test.features)\n",
    "\n",
    "    # convert to DF\n",
    "    tr = data_train.convert_to_dataframe()[0]\n",
    "    va = data_valid.convert_to_dataframe()[0]\n",
    "    te = data_test.convert_to_dataframe()[0]\n",
    "\n",
    "    # save CSV\n",
    "    tr.to_csv(data_path + 'prepared/' + data + '_' + 'scaled_' + str(fold) + '_train' + '.csv', index = None, header = True)\n",
    "    va.to_csv(data_path + 'prepared/' + data + '_' + 'scaled_' + str(fold) + '_valid' + '.csv', index = None, header = True)\n",
    "    te.to_csv(data_path + 'prepared/' + data + '_' + 'scaled_' + str(fold) + '_test'  + '.csv', index = None, header = True)\n",
    "    \n",
    "    # save pickle\n",
    "    pickle.dump(data_train, open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_train.pkl', 'wb'))\n",
    "    pickle.dump(data_valid, open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_valid.pkl', 'wb'))\n",
    "    pickle.dump(data_test,  open(data_path + 'prepared/' + data + '_scaled_' + str(fold) + '_test.pkl',  'wb'))\n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
